"""
Utilities for caching and loading reconstruction probabilities from tabular foundation models.

This module provides functions to save and load reconstruction probabilities generated by
TabPFN, TabICL, and TabDPT models. This allows experimenting with different similarity
thresholds without re-running expensive model inference.
"""
import os
import json
import pickle
import numpy as np
from typing import Dict, Any, Optional, Tuple


def save_reconstruction_probs(reconstruction_probs: np.ndarray,
                              test_descriptions: list,
                              feature_value_indices: list,
                              feature_names: list,
                              dataset_name: str,
                              method_name: str,
                              seed: int,
                              fold_idx: int,
                              output_dir: str = "out/reconstruction_probs") -> str:
    """
    Save reconstruction probabilities and metadata for a specific fold.

    Args:
        reconstruction_probs: Reconstruction probability matrix (n_queries x n_features_total)
        test_descriptions: List of test pattern descriptions
        feature_value_indices: List of feature value index mappings
        feature_names: List of feature names
        dataset_name: Name of the dataset
        method_name: Name of the method ('tabpfn', 'tabicl', 'tabdpt')
        seed: Random seed used for this run
        fold_idx: Fold index (1-based)
        output_dir: Base output directory

    Returns:
        Path to the saved file
    """
    # Create directory structure
    method_dir = os.path.join(output_dir, method_name, dataset_name)
    os.makedirs(method_dir, exist_ok=True)

    # Create filename
    base_filename = f"seed_{seed}_fold_{fold_idx}"
    probs_file = os.path.join(method_dir, f"{base_filename}_probs.npy")
    metadata_file = os.path.join(method_dir, f"{base_filename}_metadata.pkl")

    # Save reconstruction probabilities as numpy array
    np.save(probs_file, reconstruction_probs)

    # Save metadata as pickle
    metadata = {
        'test_descriptions': test_descriptions,
        'feature_value_indices': feature_value_indices,
        'feature_names': feature_names,
        'dataset_name': dataset_name,
        'method_name': method_name,
        'seed': seed,
        'fold_idx': fold_idx,
        'probs_shape': reconstruction_probs.shape
    }

    with open(metadata_file, 'wb') as f:
        pickle.dump(metadata, f)

    print(f"    Saved reconstruction probs to {probs_file}")
    print(f"    Saved metadata to {metadata_file}")

    return probs_file


def load_reconstruction_probs(dataset_name: str,
                              method_name: str,
                              seed: int,
                              fold_idx: int,
                              output_dir: str = "out/reconstruction_probs") -> Optional[Tuple[np.ndarray, Dict[str, Any]]]:
    """
    Load reconstruction probabilities and metadata for a specific fold.

    Args:
        dataset_name: Name of the dataset
        method_name: Name of the method ('tabpfn', 'tabicl', 'tabdpt')
        seed: Random seed used for this run
        fold_idx: Fold index (1-based)
        output_dir: Base output directory

    Returns:
        Tuple of (reconstruction_probs, metadata) if found, None otherwise
    """
    # Build file paths
    method_dir = os.path.join(output_dir, method_name, dataset_name)
    base_filename = f"seed_{seed}_fold_{fold_idx}"
    probs_file = os.path.join(method_dir, f"{base_filename}_probs.npy")
    metadata_file = os.path.join(method_dir, f"{base_filename}_metadata.pkl")

    # Check if files exist
    if not os.path.exists(probs_file) or not os.path.exists(metadata_file):
        return None

    # Load reconstruction probabilities
    reconstruction_probs = np.load(probs_file)

    # Load metadata
    with open(metadata_file, 'rb') as f:
        metadata = pickle.load(f)

    return reconstruction_probs, metadata


def reconstruction_probs_exist(dataset_name: str,
                               method_name: str,
                               seed: int,
                               fold_idx: int,
                               output_dir: str = "out/reconstruction_probs") -> bool:
    """
    Check if reconstruction probabilities exist for a specific fold.

    Args:
        dataset_name: Name of the dataset
        method_name: Name of the method ('tabpfn', 'tabicl', 'tabdpt')
        seed: Random seed used for this run
        fold_idx: Fold index (1-based)
        output_dir: Base output directory

    Returns:
        True if both probability and metadata files exist, False otherwise
    """
    method_dir = os.path.join(output_dir, method_name, dataset_name)
    base_filename = f"seed_{seed}_fold_{fold_idx}"
    probs_file = os.path.join(method_dir, f"{base_filename}_probs.npy")
    metadata_file = os.path.join(method_dir, f"{base_filename}_metadata.pkl")

    return os.path.exists(probs_file) and os.path.exists(metadata_file)


def get_cached_reconstruction_stats(dataset_name: str,
                                    method_name: str,
                                    n_seeds: int = 10,
                                    n_folds: int = 5,
                                    output_dir: str = "out/reconstruction_probs") -> Dict[str, Any]:
    """
    Get statistics about cached reconstruction probabilities for a dataset/method.

    Args:
        dataset_name: Name of the dataset
        method_name: Name of the method
        n_seeds: Number of seeds to check
        n_folds: Number of folds to check
        output_dir: Base output directory

    Returns:
        Dictionary with cache statistics
    """
    total_expected = n_seeds * n_folds
    cached_count = 0
    missing = []

    for seed_idx in range(n_seeds):
        for fold_idx in range(1, n_folds + 1):
            # We need to know the actual seed value, so let's assume seed sequence
            # For now, just check by index pattern
            if reconstruction_probs_exist(dataset_name, method_name, seed_idx, fold_idx, output_dir):
                cached_count += 1
            else:
                missing.append((seed_idx, fold_idx))

    return {
        'total_expected': total_expected,
        'cached_count': cached_count,
        'missing_count': len(missing),
        'coverage_pct': (cached_count / total_expected * 100) if total_expected > 0 else 0,
        'missing_list': missing[:10]  # Show first 10 missing
    }